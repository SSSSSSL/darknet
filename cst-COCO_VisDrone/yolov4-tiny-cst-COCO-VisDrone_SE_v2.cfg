[net]
# Testing
#batch=1
#subdivisions=1
# Training
batch=32
subdivisions=8
width=640
height=640
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.00261
burn_in=1000
max_batches = 20000
policy=steps
steps=16000,18000
scales=.1,.1

# 0
[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=leaky

# 1
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky

# 2
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

# 3
[route]
layers=-1
groups=2
group_id=1

# 4
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

# 5
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

# 6	->64
[route]
layers = -1,-2

# 7
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

# 8	->128
[route]
layers = -6,-1

# 9	->128
[maxpool]
size=2
stride=2

# 10
[avgpool]

# 11	->128 / 8 = 16
[connected]
output=16
activation=relu

# 12
[connected]
output=128
activation=logistic

# 13
[scale_channels]
from=-4

# 14
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

# 15	->128 * 2
[route]
layers=-1
groups=2
group_id=1

# 16
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

# 17
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

# 18	->128
[route]
layers = -1,-2

# 19
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# 20	->256
[route]
layers = -6,-1

# 21
[maxpool]
size=2
stride=2

# 22
[avgpool]

# 23	->256 / 8 = 32
[connected]
output=32
activation=relu

# 24
[connected]
output=256
activation=logistic

# 25
[scale_channels]
from=-4

# 26
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# 27
[route]
layers=-1
groups=2
group_id=1

# 28
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

# 29
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

# 30
[route]
layers = -1,-2

# 31
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# 32	->512
[route]
layers = -6,-1

# 33
[maxpool]
size=2
stride=2

# 34	->1*1*512
[avgpool]

# 35	->512 / 8 = 64
[connected]
output=64
activation=relu

# 36
[connected]
output=512
activation=logistic

# 37
[scale_channels]
from=-4

# 38
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

##################################

# 39
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# 40
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

# 41
[convolutional]
size=1
stride=1
pad=1
filters=27
activation=linear


# 42
[yolo]
mask = 3,4,5
anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319
classes=4
num=6
jitter=.3
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6

# 43
[route]
layers = -4

# 44
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# 45
[upsample]
stride=2

# 46
[route]
layers = -1, 31

# 47
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# 48
[convolutional]
size=1
stride=1
pad=1
filters=27
activation=linear

# 49
[yolo]
mask = 1,2,3
anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319
classes=4
num=6
jitter=.3
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6
